{
  "post": {
    "id": "8e81d948f8271252abc84b647efb7e8b1d335dc40b68b4dc8d47f190d60f1624",
    "title": "GPT-OSS Jailbroken",
    "excerpt": "Looks like Chat GPT’s open source model has already been jailbroken. Shipping models with safety cards doesn’t make them safe, but it does make it fun for hackers to bypass.\n\n<a href=\"https://x.com/_lyraaaa_/status/1952825311118475607?s=46&t=gbXFVtdeAqulPOfK2uLh7g\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/_lyraaaa_/status/1952825311118475607?s=46&t=gbXFVtdeAqulPOfK2uLh7g</a>\n",
    "content": "Looks like Chat GPT’s open source model has already been jailbroken. Shipping models with safety cards doesn’t make them safe, but it does make it fun for hackers to bypass.\n\n<a href=\"https://x.com/_lyraaaa_/status/1952825311118475607?s=46&t=gbXFVtdeAqulPOfK2uLh7g\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/_lyraaaa_/status/1952825311118475607?s=46&t=gbXFVtdeAqulPOfK2uLh7g</a>\n",
    "filename": "GPT-OSS Jailbroken.md",
    "slug": "gpt-oss-jailbroken-20250806",
    "created_at": "2025-08-06T15:10:05.121034",
    "updated_at": "2025-08-06T15:10:05.121034"
  },
  "last_updated": "2025-08-07T06:10:05.130807"
}